# AI Agent Setup Guide

## âœ… What's Been Created

I've set up a clean, modular AI agent system for CollabCanvas with simple atomic functions that an LLM can use. Here's what's included:

### 1. **Core Agent Actions** (`src/services/agentActions.js`)
Basic functions for manipulating shapes:

- **Query**: `getAllShapes()`, `findShapes()`, `getShapeInfo()`
- **Create**: `createShape()`, `createMultipleShapes()`, `createGrid()`
- **Delete**: `deleteShape()`, `deleteMultipleShapes()`, `deleteShapesByCriteria()`
- **Move**: `moveShapeTo()`, `moveShapeBy()`, `moveMultipleShapesBy()`
- **Modify**: `changeShapeColor()`, `resizeShape()`, `rotateShape()`, `changeText()`

### 2. **React Hook** (`src/hooks/useAgentActions.js`)
Easy-to-use hook that wraps the agent actions with shapes state.

### 3. **Agent Executor** (`src/services/agentExecutor.js`)
- Tool definitions for LLM function calling (OpenAI/Anthropic compatible)
- Function execution router
- Template for LLM API integration

### 4. **UI Component** (`src/components/AIAgentPanel.jsx`)
Beautiful, collapsible chat interface for AI agent interaction.

### 5. **Documentation** (`src/services/AI_AGENT_GUIDE.md`)
Complete guide with examples for every function.

---

## ðŸš€ How to Use It

### Quick Integration (3 steps):

#### Step 1: Add the AI Agent Panel to your Canvas

Edit `src/components/Canvas.jsx`:

```jsx
import AIAgentPanel from './AIAgentPanel';

function Canvas() {
  // ... existing code ...
  
  return (
    <div className="canvas-wrapper">
      {/* ... existing components ... */}
      
      {/* Add AI Agent Panel */}
      <AIAgentPanel shapes={shapes} />
    </div>
  );
}
```

#### Step 2: Choose Your LLM Provider

Pick one of these options:

**Option A: OpenAI (Recommended)**
```bash
npm install openai
```

Add to `.env`:
```
VITE_OPENAI_API_KEY=your-api-key-here
```

**Option B: Anthropic Claude**
```bash
npm install @anthropic-ai/sdk
```

Add to `.env`:
```
VITE_ANTHROPIC_API_KEY=your-api-key-here
```

**Option C: Any other LLM with function calling**
- Implement your own LLM call function
- Follow the pattern in `agentExecutor.js`

#### Step 3: Implement the LLM Integration

Edit `src/services/agentExecutor.js` and uncomment/modify one of these functions:

**For OpenAI:**
```javascript
export async function callOpenAI({ messages, tools }) {
  const OpenAI = require('openai');
  const openai = new OpenAI({
    apiKey: import.meta.env.VITE_OPENAI_API_KEY,
    dangerouslyAllowBrowser: true, // Use backend in production!
  });

  const response = await openai.chat.completions.create({
    model: 'gpt-4-turbo-preview',
    messages,
    tools: tools.map(tool => ({
      type: 'function',
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.parameters,
      },
    })),
    tool_choice: 'auto',
  });

  return {
    tool_calls: response.choices[0].message.tool_calls || [],
  };
}
```

**For Anthropic:**
```javascript
export async function callAnthropic({ messages, tools }) {
  const Anthropic = require('@anthropic-ai/sdk');
  const anthropic = new Anthropic({
    apiKey: import.meta.env.VITE_ANTHROPIC_API_KEY,
  });

  const response = await anthropic.messages.create({
    model: 'claude-3-opus-20240229',
    max_tokens: 4096,
    tools: tools.map(tool => ({
      name: tool.name,
      description: tool.description,
      input_schema: tool.parameters,
    })),
    messages,
  });

  const toolCalls = response.content
    .filter(block => block.type === 'tool_use')
    .map(block => ({
      function: {
        name: block.name,
        arguments: JSON.stringify(block.input),
      },
    }));

  return { tool_calls: toolCalls };
}
```

---

## ðŸ“‹ Example Commands

Once integrated, users can type commands like:

- âœ… "Create a red square at position 200, 200"
- âœ… "Create a grid of 4 circles"
- âœ… "Select the red square on the left and move it 10px to the right"
- âœ… "Change all blue rectangles to green"
- âœ… "Delete the text on the bottom"
- âœ… "Create 5 circles in a row"
- âœ… "Move all red shapes 20 pixels down"
- âœ… "Resize the largest rectangle to 200x150"

---

## ðŸ—ï¸ Architecture

```
User Command
    â†“
AIAgentPanel (UI)
    â†“
agentExecutor.processAgentCommand()
    â†“
LLM API (OpenAI/Anthropic)
    â†“ (returns function calls)
agentExecutor.executeFunctionCall()
    â†“
agentActions.* (atomic operations)
    â†“
Firebase/Firestore (real-time updates)
```

---

## ðŸ”§ Advanced Usage

### Using Agent Actions Programmatically

You can also use agent actions directly in your code:

```javascript
import { useAgentActions } from '../hooks/useAgentActions';

function MyComponent() {
  const { shapes } = useShapes();
  const agent = useAgentActions(shapes);
  
  // Example: Find and move red shapes
  const handleMoveRedShapes = async () => {
    const redShapes = agent.findShapes({ color: 'red' });
    const shapeIds = redShapes.map(s => s.id);
    await agent.moveMultipleShapesBy(shapeIds, 50, 0);
  };
  
  return (
    <button onClick={handleMoveRedShapes}>
      Move Red Shapes
    </button>
  );
}
```

### Adding Custom Functions

To add new agent actions:

1. Add function to `src/services/agentActions.js`
2. Export it from `src/hooks/useAgentActions.js`
3. Add tool definition to `AGENT_TOOLS` in `src/services/agentExecutor.js`
4. Add case to `executeFunctionCall()` switch statement

---

## ðŸ” Security Notes

**âš ï¸ IMPORTANT:** The example implementations use `dangerouslyAllowBrowser: true` for OpenAI, which exposes your API key in the frontend. This is **ONLY FOR DEVELOPMENT**.

### For Production:

**Option 1: Backend Proxy (Recommended)**
```
User â†’ Your Backend API â†’ LLM API â†’ Your Backend â†’ User
```

Create an endpoint like `/api/agent/execute`:
```javascript
// Backend (Node.js/Express)
app.post('/api/agent/execute', async (req, res) => {
  const { command, shapes } = req.body;
  
  // Validate user authentication
  // Rate limit requests
  
  const result = await processAgentCommand(
    command,
    shapes,
    callOpenAI // API key stored in backend env vars
  );
  
  res.json(result);
});
```

**Option 2: Edge Functions**
Use Vercel Edge Functions, Cloudflare Workers, or Firebase Functions to proxy LLM calls.

---

## ðŸ“Š Function Reference

### Query Functions

| Function | Purpose | Example |
|----------|---------|---------|
| `getAllShapes()` | Get all shapes | `const shapes = getAllShapes()` |
| `findShapes(criteria)` | Find by type/color/position | `findShapes({ color: 'red', position: 'left' })` |
| `getShapeInfo(id)` | Get single shape | `await getShapeInfo('shape-123')` |

### Create Functions

| Function | Purpose |
|----------|---------|
| `createShape(type, x, y, props)` | Create single shape |
| `createMultipleShapes(array)` | Create multiple shapes |
| `createGrid(type, rows, cols, x, y, spacing)` | Create shape grid |

### Delete Functions

| Function | Purpose |
|----------|---------|
| `deleteShape(id)` | Delete by ID |
| `deleteMultipleShapes(ids)` | Delete multiple by IDs |
| `deleteShapesByCriteria(criteria)` | Delete matching criteria |

### Move Functions

| Function | Purpose | Notes |
|----------|---------|-------|
| `moveShapeTo(id, x, y)` | Absolute position | - |
| `moveShapeBy(id, dx, dy)` | Relative movement | +X=right, -X=left, +Y=down, -Y=up |
| `moveMultipleShapesBy(ids, dx, dy)` | Move multiple | - |

### Modify Functions

| Function | Purpose |
|----------|---------|
| `changeShapeColor(id, color)` | Change color |
| `changeMultipleShapesColor(ids, color)` | Change multiple colors |
| `resizeShape(id, width, height)` | Resize shape |
| `rotateShape(id, degrees)` | Rotate shape |
| `changeText(id, text)` | Change text content |

---

## ðŸ§ª Testing

Test individual functions:

```javascript
// In browser console or component
const agent = useAgentActions(shapes);

// Create a shape
await agent.createShape('rectangle', 100, 100, { color: 'blue' });

// Find it
const blueShapes = agent.findShapes({ color: 'blue' });
console.log(blueShapes);

// Move it
await agent.moveShapeBy(blueShapes[0].id, 50, 0);

// Change color
await agent.changeShapeColor(blueShapes[0].id, 'green');

// Delete it
await agent.deleteShape(blueShapes[0].id);
```

---

## ðŸ› Troubleshooting

### "LLM API not configured" error
- Implement `callOpenAI` or `callAnthropic` in `agentExecutor.js`
- Add API key to `.env` file
- Restart dev server after adding env variables

### Commands not working
- Check browser console for errors
- Verify shapes are being found with `findShapes()`
- Check Firebase permissions

### Agent doesn't understand commands
- Make sure your LLM API key is valid
- Check the system prompt in `processAgentCommand()`
- Try more explicit commands

---

## ðŸ“š Next Steps

1. **Integrate LLM API** (choose OpenAI/Anthropic/other)
2. **Add AI Agent Panel to Canvas component**
3. **Test with example commands**
4. **Set up backend proxy for production**
5. **Customize the UI to match your design**
6. **Add more advanced functions as needed**

---

## ðŸŽ¯ Design Philosophy

This system follows a simple philosophy:

> **Provide simple, atomic tools. Let the LLM figure out how to use them.**

Instead of creating complex, multi-step functions, we give the AI basic building blocks. The LLM's intelligence handles:
- Natural language understanding
- Breaking complex commands into steps
- Querying for shapes before operating on them
- Error handling and retries

This makes the system:
- âœ… Easy to maintain
- âœ… Easy to extend
- âœ… Flexible for any command
- âœ… Transparent (you see what functions are called)

---

## ðŸ“ž Support

For detailed documentation on each function, see:
- `src/services/AI_AGENT_GUIDE.md` - Complete function reference
- `src/services/agentActions.js` - Implementation and JSDoc comments
- `src/services/agentExecutor.js` - LLM integration examples

Happy building! ðŸš€


